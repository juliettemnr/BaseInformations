

*Zhao, S., & Rudzicz, F. (2015, April). Classifying phonological categories in imagined and articulated speech. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 992-996). IEEE.*

**Aim** : combine EEG, facial and audio during imagined or vocalized phonemic and single-word prompts 

**Materials** : 7 phonemic syllabic and 4 words from Kent's list of phonetically-similar pairs 

**Protocole** :
- 5 seconds rest state 
- stimulus state (with 2 seconds to be in position to pronounce)
- 5 seconds imagined speech 
- speaking state 

**Classification** : 
Stimulus VS Speaking
Rest VS Imagined
Stimulus VS Imagined

*Coretto, G. A. P., Gareis, I. E., & Rufiner, H. L. (2017, January). Open access database of EEG signals recorded during imagined speech. In 12th Int Symp Med Inf Proc & Anal. SPIE.*

**Aim** : Create an openly accessible database of EEG signals acquired during imagined and pronounced speech from healthy subjects

**Materials** : two groups of words 
- Spanish vowels 
- Spanish words : possible commands 

**Protocole** : 
2 conditions : pronounced and imagined

**Classification** : 


*Nieto, N., Peterson, V., Rufiner, H. L., Kamienkowski, J. E., & Spies, R. (2022). Thinking out loud, an open-access EEG-based BCI dataset for inner speech recognition. Scientific Data, 9(1), 52.*

**Aim** : open-access multiclass electroencephalography database of inner speech commands that could be used for better understanding of the related brain mechanisms

**Materials** : 
mots utilisés : 4 directions 

**Protocole** :
3 possible conditions :
- silent speech : articulation without the sound 
- imagined speech : without any articular movements
- inner speech : internalized process in which the person thinks in pure meanings, generally associated with an auditory imagery of own inner “voice” (not thinking about the articulation)

3 conditions used : 
- inner speech
- pronounced speech 
- visualized condition : focus mentally moving the circle appearing at the center of the screen 

*Sereshkeh, A. R., Trott, R., Bricout, A., & Chau, T. (2017). EEG classification of covert speech using regularized neural networks. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 25(12), 2292-2300.*

**Aim** :  
- Is EEG reliable in off to distinguish yes and no
- session between different day 
- classification techniques 
-
**Materials** :
2 words : yes and no

**Protocole** : 2 sessions on two non consecutive days 

**Classification** : 
3 binary classifiers,:
- yes VS rest 
- no VS rest 
- yes VS no


-----------------------------------------------------------
Brain area elicited during covert speech : Wise et al. : left posterior superior temporal cortex (Wernicke's area), left prefrontal and premoto region (including Broca's area) + supplementary motor area 

